{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdf2image tqdm boto3\n",
    "!pip install numpy Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Normalize PDFs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List all PDF files in current directory\n",
    "pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in tqdm(pdf_files, desc='Converting PDFs to PNGs'):\n",
    "    base_name = os.path.splitext(pdf_file)[0]\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    images = convert_from_path(pdf_file)\n",
    "    \n",
    "    # Save each page as PNG\n",
    "    for i, image in enumerate(images, start=1):\n",
    "        output_file = f'{base_name}_{i:02d}.png'\n",
    "        image.save(output_file, 'PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Extract content from all PNG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Initialize AWS Textract client\n",
    "textract = boto3.client('textract', region_name='us-east-1')\n",
    "\n",
    "def get_base_filename(filename):\n",
    "    # Remove page number suffix if it exists (e.g., '_01', '_02')\n",
    "    return re.sub(r'_\\d+$', '', os.path.splitext(filename)[0])\n",
    "\n",
    "# Get all PNG files\n",
    "png_files = [f for f in os.listdir('.') if f.endswith('.png')]\n",
    "\n",
    "# Group files by base name\n",
    "file_groups = {}\n",
    "for png_file in png_files:\n",
    "    base_name = get_base_filename(png_file)\n",
    "    if base_name not in file_groups:\n",
    "        file_groups[base_name] = []\n",
    "    file_groups[base_name].append(png_file)\n",
    "\n",
    "# Process each group of files\n",
    "for base_name, files in tqdm(file_groups.items(), desc='Processing PNG files'):\n",
    "    output_txt = f'{base_name}.txt'\n",
    "    \n",
    "    # Skip if TXT file already exists\n",
    "    if os.path.exists(output_txt):\n",
    "        continue\n",
    "    \n",
    "    all_text = []\n",
    "    \n",
    "    # Sort files to ensure correct page order\n",
    "    files.sort()\n",
    "    \n",
    "    for png_file in files:\n",
    "        with open(png_file, 'rb') as image:\n",
    "            # Call Textract\n",
    "            response = textract.detect_document_text(\n",
    "                Document={'Bytes': image.read()}\n",
    "            )\n",
    "            \n",
    "            # Extract text from response\n",
    "            page_text = '\\n'.join([item['Text'] for item in response['Blocks'] if item['BlockType'] == 'LINE'])\n",
    "            all_text.append(page_text)\n",
    "    \n",
    "    # Write combined text to file\n",
    "    with open(output_txt, 'w') as f:\n",
    "        f.write('\\n'.join(all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Process all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Bedrock client\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "def process_text_with_bedrock(text_content):\n",
    "    prompt = f\"Go through this transcript of an electric bill and parse all the relevant information (name, address, phone, usage, cost, etc) into a JSON in the following format:\\n\\n{{\\\n",
    "  \\\"name\\\": \\\"<customer name>\\\",\\\n",
    "  \\\"account\\\": \\\"<account number>\\\",\\\n",
    "  \\\"address\\\": \\\"<address broken down into sub-properties for address, city, zip, etc>\\\",\\\n",
    "  \\\"phone\\\": \\\"<phone number>\\\",\\\n",
    "  \\\"email\\\": \\\"<customer email>\\\",\\\n",
    "  \\\"dueDate\\\": \\\"<Due date in YYYY-MM-DD format>\\\",\\\n",
    "  \\\"amount\\\": <amount in number format>,\\\n",
    "  \\\"usage\\\": <total kWh usage in number format>\\\n",
    "}}\\n\\nDon't include anything on the response other than the JSON.\\n\\nHere's the content:\\n{text_content}\"\n",
    "\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        \"inferenceConfig\": {\"maxTokens\": 2048, \"topP\": 0.9, \"topK\": 20, \"temperature\": 0.7}\n",
    "    }\n",
    "\n",
    "    response = client.invoke_model_with_response_stream(\n",
    "        modelId=MODEL_ID,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\", {}).get(\"text\", \"\")\n",
    "\n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        start_idx = full_response.find('{')\n",
    "        end_idx = full_response.rfind('}') + 1\n",
    "        json_str = full_response[start_idx:end_idx]\n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get all TXT files\n",
    "txt_files = [f for f in os.listdir('.') if f.endswith('.txt')]\n",
    "\n",
    "# Process each TXT file\n",
    "for txt_file in tqdm(txt_files, desc='Processing text files'):\n",
    "    json_file = f\"{os.path.splitext(txt_file)[0]}.json\"\n",
    "    \n",
    "    # Skip if JSON file already exists\n",
    "    if os.path.exists(json_file):\n",
    "        continue\n",
    "    \n",
    "    # Read text content\n",
    "    with open(txt_file, 'r') as f:\n",
    "        text_content = f.read()\n",
    "    \n",
    "    # Process with Bedrock\n",
    "    result = process_text_with_bedrock(text_content)\n",
    "    \n",
    "    if result:\n",
    "        # Save JSON result\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(result, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}